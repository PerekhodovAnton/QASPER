{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "args = dict(\n",
    "  stage=\"sft\",                        # do supervised fine-tuning\n",
    "  do_train=True,\n",
    "  model_name_or_path=\"/home/asperekhodov/.cache/huggingface/hub/models--lightblue--suzume-llama-3-8B-multilingual/snapshots/a91a26333d22b2382025e4e1f5a7869142a683c3\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  dataset=\"QasperQAInstruct, QasperSumInstruct, CLSumInstruct, CLQAInstruct\",             # use alpaca and identity datasets\n",
    "  template=\"llama3\",                     # use llama3 prompt template\n",
    "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  output_dir=\"suzume_lora2\",                  # the path to save LoRA adapters\n",
    "  per_device_train_batch_size=2,               # the batch size\n",
    "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  warmup_ratio=0.1,                      # use warmup scheduler\n",
    "  save_steps=100000,                      # save checkpoint every 1000 steps\n",
    "  learning_rate=1e-5,                     # the learning rate\n",
    "  num_train_epochs=1.0,                    # the epochs of training\n",
    "  max_samples=100000, \n",
    "  max_grad_norm=1.0,\n",
    "  loraplus_lr_ratio=16.0,\n",
    "  fp16=True,                         # use float16 mixed precision training\n",
    ")\n",
    "\n",
    "json.dump(args, open(\"train_suzume2.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
    "\n",
    "!~/.local/bin/llamafactory-cli train train_suzume2.json"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
